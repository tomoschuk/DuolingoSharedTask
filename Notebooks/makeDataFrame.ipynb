{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook reads in the Spanish, English, & Frech data, does a bunch of processing, and pickles the results for use by other (modeling) notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## \n",
    "# Controls: use this cell to control global variables\n",
    "langs = [x.lower() for x in ['En','Sp','Fr']] # which languages' data to load? (handles case)\n",
    "subset_size = 'all' # how many rows (from each data set specified above) should we take (useful for debug)\n",
    "#subset_size = 25000 # how many rows (from each data set specified above) should we take (useful for debug)\n",
    "write_out = True # do we want to pickle the data for later use? (not currently implemented)\n",
    "write_fn = 'EnEsFr_Data.pickle' # not currently implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomoschuk/anaconda/lib/python3.6/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "# Original libraries\n",
    "## with JL annotations\n",
    "import argparse ## for dealing with command line args (it seems)\n",
    "from collections import defaultdict, namedtuple ## useful data structs (defaultdict provides result if !exists key)\n",
    "from io import open ## input/output \n",
    "import math ## duh\n",
    "import os ## navigating the os\n",
    "from random import shuffle, uniform ## duh\n",
    "\n",
    "from future.builtins import range \n",
    "from future.utils import iteritems\n",
    "\n",
    "# Sigma is the L2 prior variance, regularizing the baseline model. Smaller sigma means more regularization.\n",
    "_DEFAULT_SIGMA = 20.0\n",
    "\n",
    "# Eta is the learning rate/step size for SGD. Larger means larger step size.\n",
    "_DEFAULT_ETA = 0.1\n",
    "\n",
    "\n",
    "#Brendan and Jarrett libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from googletrans import Translator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle # save data to file (to avoid having to load 2mil lines every time)\n",
    "import numpy as np\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics import edit_distance\n",
    "from scipy.stats import zscore\n",
    "from tqdm import tqdm # progress bar\n",
    "#import warnings\n",
    "#warnings.simplefilter(\"ignore\")\n",
    "\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     90
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    This method loads and returns the data in filename. If the data is labelled training data, it returns labels too.\n",
    "\n",
    "    Parameters:\n",
    "        filename: the location of the training or test data you want to load.\n",
    "\n",
    "    Returns:\n",
    "        data: a list of InstanceData objects from that data type and track.\n",
    "        labels (optional): if you specified training data, a dict of instance_id:label pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    # 'data' stores a list of 'InstanceData's as values.\n",
    "    data = []\n",
    "\n",
    "    # If this is training data, then 'labels' is a dict that contains instance_ids as keys and labels as values.\n",
    "    training = False\n",
    "    if filename.find('train') != -1:\n",
    "        training = True\n",
    "\n",
    "    if training:\n",
    "        labels = dict()\n",
    "\n",
    "    num_exercises = 0\n",
    "    print('Loading instances...')\n",
    "\n",
    "    with open(filename, 'rt') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            # If there's nothing in the line, then we're done with the exercise. Print if needed, otherwise continue\n",
    "            if len(line) == 0:\n",
    "                num_exercises += 1\n",
    "                if num_exercises % 100000 == 0:\n",
    "                    print('Loaded ' + str(len(data)) + ' instances across ' + str(num_exercises) + ' exercises...')\n",
    "\n",
    "            # If the line starts with #, then we're beginning a new exercise\n",
    "            elif line[0] == '#':\n",
    "                list_of_exercise_parameters = line[2:].split()\n",
    "                instance_properties = dict()\n",
    "                for exercise_parameter in list_of_exercise_parameters:\n",
    "                    [key, value] = exercise_parameter.split(':')\n",
    "                    if key == 'countries':\n",
    "                        value = value.split('|')\n",
    "                    elif key == 'days':\n",
    "                        value = float(value)\n",
    "                    elif key == 'time':\n",
    "                        if value == 'null':\n",
    "                            value = None\n",
    "                        else:\n",
    "                            assert '.' not in value\n",
    "                            value = int(value)\n",
    "                    instance_properties[key] = value\n",
    "\n",
    "            # Otherwise we're parsing a new Instance for the current exercise\n",
    "            else:\n",
    "                line = line.split()\n",
    "                if training:\n",
    "                    assert len(line) == 7\n",
    "                else:\n",
    "                    assert len(line) == 6\n",
    "                assert len(line[0]) == 12\n",
    "\n",
    "                instance_properties['instance_id'] = line[0]\n",
    "\n",
    "                instance_properties['token'] = line[1]\n",
    "                instance_properties['part_of_speech'] = line[2]\n",
    "\n",
    "                instance_properties['morphological_features'] = dict()\n",
    "                for l in line[3].split('|'):\n",
    "                    [key, value] = l.split('=')\n",
    "                    if key == 'Person':\n",
    "                        value = int(value)\n",
    "                    instance_properties['morphological_features'][key] = value\n",
    "\n",
    "                instance_properties['dependency_label'] = line[4]\n",
    "                instance_properties['dependency_edge_head'] = int(line[5])\n",
    "                if training:\n",
    "                    label = float(line[6])\n",
    "                    labels[instance_properties['instance_id']] = label\n",
    "                data.append(InstanceData(instance_properties=instance_properties))\n",
    "\n",
    "        print('Done loading ' + str(len(data)) + ' instances across ' + str(num_exercises) +\n",
    "              ' exercises.\\n')\n",
    "\n",
    "    if training:\n",
    "        return data, labels\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "class InstanceData(object):\n",
    "    \"\"\"\n",
    "    A bare-bones class to store the included properties of each instance. This is meant to act as easy access to the\n",
    "    data, and provides a launching point for deriving your own features from the data.\n",
    "    \"\"\"\n",
    "    def __init__(self, instance_properties):\n",
    "\n",
    "        # Parameters specific to this instance\n",
    "        self.instance_id = instance_properties['instance_id']\n",
    "        self.token = instance_properties['token']\n",
    "        self.part_of_speech = instance_properties['part_of_speech']\n",
    "        self.morphological_features = instance_properties['morphological_features']\n",
    "        self.dependency_label = instance_properties['dependency_label']\n",
    "        self.dependency_edge_head = instance_properties['dependency_edge_head']\n",
    "\n",
    "        # Derived parameters specific to this instance\n",
    "        self.exercise_index = int(self.instance_id[8:10])\n",
    "        self.token_index = int(self.instance_id[10:12])\n",
    "\n",
    "        # Derived parameters specific to this exercise\n",
    "        self.exercise_id = self.instance_id[:10]\n",
    "\n",
    "        # Parameters shared across the whole session\n",
    "        self.user = instance_properties['user']\n",
    "        self.countries = instance_properties['countries']\n",
    "        self.days = instance_properties['days']\n",
    "        self.client = instance_properties['client']\n",
    "        self.session = instance_properties['session']\n",
    "        self.format = instance_properties['format']\n",
    "        self.time = instance_properties['time']\n",
    "\n",
    "        # Derived parameters shared across the whole session\n",
    "        self.session_id = self.instance_id[:8]\n",
    "        \n",
    "    def to_features(self):\n",
    "        \"\"\"\n",
    "        Prepares those features that we wish to use in the LogisticRegression example in this file. We introduce a bias,\n",
    "        and take a few included features to use. Note that this dict restructures the corresponding features of the\n",
    "        input dictionary, 'instance_properties'.\n",
    "\n",
    "        Returns:\n",
    "            to_return: a representation of the features we'll use for logistic regression in a dict. A key/feature is a\n",
    "                key/value pair of the original 'instance_properties' dict, and we encode this feature as 1.0 for 'hot'.\n",
    "        \"\"\"\n",
    "        to_return = dict()\n",
    "\n",
    "        to_return['bias'] = 1.0\n",
    "        to_return['user:' + self.user] = 1.0\n",
    "        to_return['format:' + self.format] = 1.0\n",
    "        to_return['token:' + self.token.lower()] = 1.0\n",
    "\n",
    "        to_return['part_of_speech:' + self.part_of_speech] = 1.0\n",
    "        for morphological_feature in self.morphological_features:\n",
    "            to_return['morphological_feature:' + morphological_feature] = 1.0\n",
    "        to_return['dependency_label:' + self.dependency_label] = 1.0\n",
    "\n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a dataset in duolingo format and a set of labels for each, produce a pandas df\n",
    "def makeDF(instances, labels = 0, train = True):\n",
    "    data = []\n",
    "    for i in range(0,len(instances)):\n",
    "        data.append(\n",
    "            {'user': instances[i].user,\n",
    "            'countries': ' '.join(instances[i].countries),\n",
    "            'days': instances[i].days,\n",
    "            'client': instances[i].client,\n",
    "            'session': instances[i].session,\n",
    "            'sessionID': instances[i].session_id,\n",
    "            'format': instances[i].format,\n",
    "            'time': instances[i].time,\n",
    "            'exerciseID': instances[i].exercise_id,\n",
    "            'instanceID': instances[i].instance_id,\n",
    "            'tokenIndex': instances[i].token_index,\n",
    "            'token': instances[i].token,\n",
    "            'dependencyLbl': instances[i].dependency_label,\n",
    "            'dependencyEdgeHead': instances[i].dependency_edge_head,\n",
    "            'pos': instances[i].part_of_speech,\n",
    "            'morpho': instances[i].morphological_features})\n",
    "        hashID = data[i]['instanceID']\n",
    "        if train == True:\n",
    "            data[i]['error'] = labels[hashID]\n",
    "    return(pd.DataFrame(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loading English data...\n",
      "Loading instances...\n",
      "Loaded 317049 instances across 100000 exercises...\n",
      "Loaded 635368 instances across 200000 exercises...\n",
      "Loaded 951536 instances across 300000 exercises...\n",
      "Loaded 1271940 instances across 400000 exercises...\n",
      "Loaded 1591345 instances across 500000 exercises...\n",
      "Loaded 1911213 instances across 600000 exercises...\n",
      "Loaded 2227445 instances across 700000 exercises...\n",
      "Loaded 2546705 instances across 800000 exercises...\n",
      "Done loading 2622958 instances across 824012 exercises.\n",
      "\n",
      "    Loading English dev data...\n",
      "Loading instances...\n",
      "Loaded 334439 instances across 100000 exercises...\n",
      "Done loading 387374 instances across 115770 exercises.\n",
      "\n",
      "    Loading English dev key data...\n",
      "    Loading English test data...\n",
      "Loading instances...\n",
      "Loaded 337728 instances across 100000 exercises...\n",
      "Done loading 386604 instances across 114586 exercises.\n",
      "\n",
      "    Taking subset of size = 25000 rows... Complete.\n",
      "    Converting to a pandas dataframe ... Complete.\n",
      "Done loading English data!\n",
      "\n",
      "    Loading Spanish data...\n",
      "Loading instances...\n",
      "Loaded 266882 instances across 100000 exercises...\n",
      "Loaded 537453 instances across 200000 exercises...\n",
      "Loaded 804717 instances across 300000 exercises...\n",
      "Loaded 1075885 instances across 400000 exercises...\n",
      "Loaded 1348071 instances across 500000 exercises...\n",
      "Loaded 1620766 instances across 600000 exercises...\n",
      "Loaded 1887020 instances across 700000 exercises...\n",
      "Done loading 1973558 instances across 731896 exercises.\n",
      "\n",
      "    Loading Spanish dev data...\n",
      "Loading instances...\n",
      "Done loading 288864 instances across 96003 exercises.\n",
      "\n",
      "    Loading Spanish dev key data...\n",
      "    Loading Spanish test data...\n",
      "Loading instances...\n",
      "Done loading 282181 instances across 93145 exercises.\n",
      "\n",
      "    Taking subset of size = 25000 rows... Complete.\n",
      "    Converting to a pandas dataframe ... Complete.\n",
      "Done loading Spanish data!\n",
      "\n",
      "    Loading French data...\n",
      "Loading instances...\n",
      "Loaded 285973 instances across 100000 exercises...\n",
      "Loaded 567856 instances across 200000 exercises...\n",
      "Loaded 850511 instances across 300000 exercises...\n",
      "Done loading 926657 instances across 326792 exercises.\n",
      "\n",
      "    Loading French dev data...\n",
      "Loading instances...\n",
      "Done loading 137571 instances across 43610 exercises.\n",
      "\n",
      "    Loading French dev key data...\n",
      "    Loading French test data...\n",
      "Loading instances...\n",
      "Done loading 135525 instances across 41753 exercises.\n",
      "\n",
      "    Taking subset of size = 25000 rows... Complete.\n",
      "    Converting to a pandas dataframe ... Complete.\n",
      "Done loading French data!\n",
      "\n",
      "Done Loading Data!\n"
     ]
    }
   ],
   "source": [
    "## Load in the data using their pre-existing function & take subset if desired & pandafy\n",
    "\n",
    "# function mapping l2 abbrev to name of L2 and where to find that data\n",
    "def get_filepath(l2):\n",
    "    l2 = l2.lower()[0:2] # make sure we have the lowercase first 2 letters of the language name\n",
    "    return({\n",
    "            'sp': {'name':'Spanish', 'path':'../Data/data_es_en/es_en.slam.20171218.train'}, # L1 English L2 Spanish\n",
    "            'en': {'name':'English', 'path':'../Data/data_en_es/en_es.slam.20171218.train'}, # L1 Spanish L2 English\n",
    "            'fr': {'name':'French',  'path':'../Data/data_fr_en/fr_en.slam.20171218.train'}}[l2]) # L1 English L2 French\n",
    "def get_dev_filepath(l2):\n",
    "    l2 = l2.lower()[0:2] # make sure we have the lowercase first 2 letters of the language name\n",
    "    return({\n",
    "            'sp': {'name':'Spanish', 'path':'../Data/data_es_en/es_en.slam.20171218.dev'}, # L1 English L2 Spanish\n",
    "            'en': {'name':'English', 'path':'../Data/data_en_es/en_es.slam.20171218.dev'}, # L1 Spanish L2 English\n",
    "            'fr': {'name':'French',  'path':'../Data/data_fr_en/fr_en.slam.20171218.dev'}}[l2]) # L1 English L2 French\n",
    "def get_dev_key_filepath(l2):\n",
    "    l2 = l2.lower()[0:2] # make sure we have the lowercase first 2 letters of the language name\n",
    "    return({\n",
    "            'sp': {'name':'Spanish', 'path':'../Data/data_es_en/es_en.slam.20171218.dev.key'}, # L1 English L2 Spanish\n",
    "            'en': {'name':'English', 'path':'../Data/data_en_es/en_es.slam.20171218.dev.key'}, # L1 Spanish L2 English\n",
    "            'fr': {'name':'French',  'path':'../Data/data_fr_en/fr_en.slam.20171218.dev.key'}}[l2]) # L1 English L2 French\n",
    "def get_test_filepath(l2):\n",
    "    l2 = l2.lower()[0:2] # make sure we have the lowercase first 2 letters of the language name\n",
    "    return({\n",
    "            'sp': {'name':'Spanish', 'path':'../Data/data_es_en/test.es_en'}, # L1 English L2 Spanish\n",
    "            'en': {'name':'English', 'path':'../Data/data_en_es/test.en_es'}, # L1 Spanish L2 English\n",
    "            'fr': {'name':'French',  'path':'../Data/data_fr_en/test.fr_en'}}[l2]) # L1 English L2 French\n",
    "# Read in the data for desired language(s) and save it in a dictionary\n",
    "data = {}\n",
    "devData = {}\n",
    "testData = {}\n",
    "for l2 in langs:\n",
    "    l2_info = get_filepath(l2)\n",
    "    l2_name = l2_info['name']\n",
    "    l2_path = l2_info['path']\n",
    "    print('    Loading ' + l2_name + ' data...')\n",
    "    Dat, Labs = load_data(l2_path)\n",
    "    \n",
    "    l2_info = get_dev_filepath(l2)\n",
    "    l2_name = l2_info['name']\n",
    "    l2_path = l2_info['path']\n",
    "    print('    Loading ' + l2_name + ' dev data...')\n",
    "    devDat = load_data(l2_path)\n",
    "    \n",
    "    l2_info = get_dev_key_filepath(l2)\n",
    "    l2_name = l2_info['name']\n",
    "    l2_path = l2_info['path']\n",
    "    print('    Loading ' + l2_name + ' dev key data...')\n",
    "    devKey = pd.read_csv(l2_path, sep = \" \", header = None)\n",
    "    devKey.columns = ['instanceID', 'error']\n",
    "    \n",
    "    l2_info = get_test_filepath(l2)\n",
    "    l2_name = l2_info['name']\n",
    "    l2_path = l2_info['path']\n",
    "    print('    Loading ' + l2_name + ' test data...')\n",
    "    testDat = load_data(l2_path)\n",
    "    \n",
    "    print('    Taking subset of size = ' + str(subset_size) + ' rows...', end = '')\n",
    "    # take subset of data (easy)\n",
    "    if subset_size != 'all':   \n",
    "        Dat = Dat[0:subset_size]\n",
    "        devDat = devDat[0:subset_size]\n",
    "        testDat = testDat[0:subset_size]\n",
    "    # find corresponding labels\n",
    "    ids = []\n",
    "    for t in Dat:\n",
    "        ids.append(t.instance_id)\n",
    "    Labs = {k: Labs[k] for k in (ids)}\n",
    "    print(' Complete.')\n",
    "    \n",
    "    print('    Converting to a pandas dataframe ...', end ='')\n",
    "    data[l2] = makeDF(Dat, Labs)\n",
    "    data[l2]['source'] = 'train'\n",
    "    devData[l2] = makeDF(devDat, train = False)\n",
    "    devData[l2] = devData[l2].merge(devKey, on = \"instanceID\", how = 'left')\n",
    "    devData[l2]['source'] = 'dev'\n",
    "    testData[l2] = makeDF(testDat, train = False)\n",
    "    testData[l2]['source'] = 'test'\n",
    "    data[l2] = pd.concat([data[l2],devData[l2],testData[l2]],ignore_index = True)\n",
    "    Dat, Labs,devDat,testDat = None, None, None, None# lighten huge memory load\n",
    "    print( ' Complete.')\n",
    "    print('Done loading ' + l2_name + ' data!\\n')\n",
    "\n",
    "print('Done Loading Data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data\n",
    "\n",
    "This is where we engineer our own features. These apply at different levels:\n",
    "\n",
    "(fill this out)\n",
    "- Dataset:\n",
    "- User:\n",
    "- Exercise:\n",
    "    + sentence length\n",
    "- Instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function defs ### \n",
    "# A function that takes a df representing a single exercise (sentence) and processes it\n",
    "def prepExerDat(ed):\n",
    "    pd.options.mode.chained_assignment = None  # default='warn' # this disables the pink warnings...\n",
    "    nrows = ed.shape[0] # count the number of rows\n",
    "    ed['sentLength'] = nrows\n",
    "    ed['timePerToken'] = ed['time']/ed['sentLength']\n",
    "    #ed['nErrorsSoFarSent'] = ed['error'].cumsum() - ed['error']\n",
    "    #ed['madeErrorYet'] =  ed['nErrorsSoFarSent'] > 0\n",
    "    pd.options.mode.chained_assignment = 'warn'  # default='warn' # reset the stupid warning thing\n",
    "    return(ed) \n",
    "\n",
    "# A function that takes a df representing (all of) a single user's data and processes it\n",
    "def prepUserDat(ud, morphos):\n",
    "    pd.options.mode.chained_assignment = None  # default='warn' # this disables the pink warnings...\n",
    "    nrows = ud.shape[0]\n",
    "    #acc = 1 - ud['error'] # need to convert from error to accuracy (a = 1- e)\n",
    "    \n",
    "    ### do stuff that affects the whole user (e.g. mean accuracies, etc. )\n",
    "    ud['userTrial'] = np.arange(1,nrows+1) # what instance number is this for this user\n",
    "\n",
    "    # user \"random effect\": collapse over everything but ... \n",
    "    groupUser = ['format','session','client']\n",
    "    #ud['user_RF'] = ud.groupby(groupUser).mean().reset_index()['error']\n",
    "    temp = ud.loc[(ud['source']=='train') | (ud['source']=='dev')].groupby(groupUser).mean().reset_index()[groupUser + ['error']]\\\n",
    "        .rename(index=str, columns = {'error': 'userErr_RF'})    \n",
    "    temp['userErrVar_RF'] = temp['userErr_RF'] * (1-temp['userErr_RF'])\n",
    "    ud = ud.merge(temp, on = groupUser, how ='left')\n",
    "    \n",
    "    # Spacing stuff\n",
    "    ud['nthOccurance'] = ud.groupby(['token']).cumcount() + 1 # repetitions per token (per user)\n",
    "    # token spacing\n",
    "    ud['tokenLag1'] = ud.groupby('token')['days'].diff() # gap between this occurrance and the one before it (1-back)\n",
    "    ud['tokenLag2'] = ud.groupby('token')['days'].diff(2) - ud['tokenLag1'] # gap between this occurrance and the one two before it (2-back)\n",
    "    # stem spacing\n",
    "    ud['stemLag1'] = ud.groupby('stem')['days'].diff() # gap between this occurrance and the one before it (1-back)\n",
    "    ud['stemLag2'] = ud.groupby('stem')['days'].diff(2) - ud['stemLag1'] # gap between this occurrance and the one two before it (2-back)\n",
    "    # morpho spacing\n",
    "    morphoGroup = ['Number','Person','Tense','VerbForm']\n",
    "    morphoLagGroup = list(set(morphoGroup).intersection(morphos))\n",
    "    ud['morphoLag1'] = ud.groupby(morphoLagGroup)['days'].diff() # gap between this occurrance and the one before it (1-back)\n",
    "    ud['morphoLag2'] = ud.groupby(morphoLagGroup)['days'].diff(2) - ud['morphoLag1']\n",
    "    \n",
    "    # lag btwn 1st and 2nd occurance:\n",
    "    # not all tokens occur twice so error could be thrown if not careful - get counts\n",
    "    tokenCounts = ud.groupby('stem').size().reset_index(name='counts')\n",
    "    multiTokens = tokenCounts.loc[tokenCounts['counts']>1] # get subset of tokens that occur more than once\n",
    "    temp = ud.loc[(ud['stem'].isin(multiTokens['stem']))]\n",
    "    temp['lagTr1Tr2'] = temp.groupby('stem')['days'].transform(lambda x: x.iloc[1]) -\\\n",
    "        temp.groupby('stem')['days'].transform(lambda x: x.iloc[0])\n",
    "    ud = ud.merge(temp[['stem','lagTr1Tr2']], on = 'stem', how='left').drop_duplicates()\n",
    "    temp=None\n",
    "\n",
    " \n",
    "    \n",
    "    #Previous exercise\n",
    "    temp = ud.groupby(['exerciseID','format']).first().reset_index().sort_values(by=['origRow_doNotUse'])\n",
    "    temp['prevFormat'] = temp.format.shift(1)\n",
    "    ud = ud.merge(temp[['exerciseID','prevFormat']], on = 'exerciseID', how = 'left')\n",
    "    temp = None\n",
    "    \n",
    "    # encode categorical feature interactions -- to be one-hot-encoded later\n",
    "    ud['format:prevFormat'] = ud['format'] + ':' + ud['prevFormat']\n",
    "    ud['format:client'] = ud['format'] + ':' + ud['client']\n",
    "\n",
    "    # do per-exercise stuff    \n",
    "    #_, idx = np.unique(ud['exerciseID'], return_index=True) # not sure what _ does\n",
    "    exercises = np.unique(ud['exerciseID'])\n",
    "    to_return = []\n",
    "    #for i in tqdm(range(len(exercises))):\n",
    "    for i in range(len(exercises)):\n",
    "        to_return.append(\n",
    "            prepExerDat(ud[ud['exerciseID'] == exercises[i]])) # process this exercise and add it to a list\n",
    "    pd.options.mode.chained_assignment = 'warn'  # default='warn' # reset the stupid warning thing\n",
    "\n",
    "    return(pd.concat(to_return))\n",
    "\n",
    "def getLangName(l2):\n",
    "    return({\n",
    "            'sp': 'spanish',\n",
    "            'fr': 'french',\n",
    "            'en': 'english'\n",
    "        }[l2]\n",
    "          )\n",
    "#Check for interlingual homographs\n",
    "def check_in_words(x):\n",
    "    if x in words.words():\n",
    "        return 1\n",
    "    else: return 0\n",
    "    \n",
    "def interaction(df,cols):\n",
    "    for tup in cols:\n",
    "        df[\":\".join(tup)] = df[tup].prod(axis=1, skipna=False)\n",
    "    return df\n",
    "                                       \n",
    "def catInteraction(df,cols):\n",
    "    for tup in cols:\n",
    "        temp = pd.get_dummies(df[tup[0]],prefix = tup[1], prefix_sep = ':{'+tup[0]+'}')\n",
    "        df = df.join(temp.multiply(df[tup[1]], axis=\"index\"))\n",
    "    return df \n",
    "\n",
    "# A function that takes a full langauge's df as input and adds the various columns we'll want \n",
    "def prepData(td, l2): # call the arg 'td 'for compatability with existing code \n",
    "    lang = getLangName(l2)\n",
    "    stemmer = SnowballStemmer(lang) #Set stemmer language\n",
    "    if lang == 'french':\n",
    "        morphos = ['Definite','Gender','Mood','Number','Person','PronType','Tense','VerbForm'] #french\n",
    "    elif lang == 'spanish': \n",
    "        morphos = ['Case','Definite','Degree','Foreign','NumType','Gender','Mood','Number','Person','Polite','Poss','PrepCase','PronType','Reflex','Tense','VerbForm'] #spanish\n",
    "    elif lang == 'english':\n",
    "        morphos = ['Case','Definite','Degree','Gender','Mood','Number','NumType'] #english\n",
    "    else: print(\"Error in building morphological list\")\n",
    "\n",
    "    print('Processing users learning ' + lang + '...')\n",
    "    print('   Adding dataframe-level features ...', end ='')\n",
    "    td['stem'] = [stemmer.stem(x) for x in td.token] # Add column for word stem\n",
    "    # do stuff that affects the whole df (e.g. split morpho col)\n",
    "    # recode client (b/c we don't care about android vs. ios diff)\n",
    "    replaceClient = {'client': {'ios': 'mobile',\n",
    "                           'android':'mobile'}}\n",
    "    td.replace(replaceClient, inplace = True)\n",
    "    \n",
    "    td['origRow_doNotUse'] = np.arange(1, td.shape[0]+1) # to make sure no shuffling is going on\n",
    "    td = pd.concat([td.drop(['morpho'], axis=1),td['morpho'].apply(pd.Series)], axis=1)\n",
    "    td['token'] = td['token'].str.lower()\n",
    "    #Get word length for each token\n",
    "    td['wordLength'] = [len(x) for x in td.token]\n",
    "    lexGroupToken = ['format','token', 'stem', 'pos']\n",
    "    #Calculate 'random effects' across tokens\n",
    "    temp = td.loc[(td['source']=='train') | (td['source']=='dev')].groupby(lexGroupToken)\\\n",
    "        .mean().reset_index()[lexGroupToken + ['error']]\\\n",
    "        .rename(index=str, columns = {'error': 'tokenErr_RF'})\n",
    "    temp['tokenErrVar_RF'] = temp['tokenErr_RF'] * (1-temp['tokenErr_RF'])\n",
    "    td = td.merge(temp, on = lexGroupToken, how='left')\n",
    "    temp = None\n",
    "    print('Complete. \\nNow processing user- & exercise-level data...')\n",
    "    # do per-user stuff: \n",
    "    # found below trick here: https://stackoverflow.com/questions/15637336/numpy-unique-with-order-preserved\n",
    "    #_, idx = np.unique(td['user'], return_index=True) # not sure what _ does\n",
    "    users = np.unique(td['user']) #<-- might not preserve order?\n",
    "    to_return = []\n",
    "    for i in range(len(users)):\n",
    "    #split up users and process them one at a time\n",
    "        to_return.append(\n",
    "            prepUserDat(td.loc[td.user == users[i]], morphos)) # process user's dat & add it to a list\n",
    "    print('Merging users\\' data & gathering orthographic info... ')\n",
    "    td = pd.concat(to_return)\n",
    "    orthoinfo = pd.read_csv('../Data/'+lang+'.csv', encoding='ISO-8859-1')\n",
    "    td = td.merge(orthoinfo, on='token', how='left')\n",
    "    \n",
    "    print('Calculating morphological complexity')\n",
    "    temp = td[morphos]\n",
    "    td['morphoComplexity'] = (len(temp.columns) - temp.isnull().sum(axis=1))/len(temp.columns)\n",
    "    temp = None\n",
    "\n",
    "    temp = pd.read_csv('../Data/'+lang+'Dict.csv', encoding='utf-16')\n",
    "    temp['token'] = temp['token_lower']\n",
    "    td = td.merge(temp.drop(['token_lower'], axis=1), on='token', how='left')\n",
    "    temp = None\n",
    "    \n",
    "    print('Transforming, imputing missing values, and z-scoring numeric columns...', end = '')\n",
    "    # Log transform where appropriate\n",
    "    sec = 1/60/60/24 # one second in units of days -- many zeros in lags so add 1 sec to each to take log\n",
    "    td['tokenLag1'] = np.log10(td['tokenLag1'] + sec)\n",
    "    td['tokenLag2'] = np.log10(td['tokenLag2'] + sec)\n",
    "    td['stemLag1'] = np.log10(td['stemLag1'] + sec)\n",
    "    td['stemLag2'] = np.log10(td['stemLag2'] + sec)\n",
    "    #td['morphoLag1'] = np.log10(td['morphoLag1'] + sec)\n",
    "    #td['morphoLag2'] = np.log10(td['morphoLag2'] + sec) \n",
    "    td['logWordFreq'] = np.log10(td['WordFreq'])\n",
    "    td['log?'] = np.log10(td['?'])\n",
    "    td['log?Pho'] = np.log10(td['?Pho'])\n",
    "    td['log?EngPho'] = np.log10(td['?EngPho']) # is it still called that in the English data?\n",
    "\n",
    "    #replace NaNs in continuous columns with the mean & z-score them\n",
    "    continuousCols = list(td.select_dtypes(include=[np.number]))\n",
    "    continuousCols.remove('error')\n",
    "    #continuousCols.remove('acc')\n",
    "    continuousCols.remove('Homograph')\n",
    "    continuousCols.remove('origRow_doNotUse')\n",
    "    td[continuousCols] = td[continuousCols] \\\n",
    "       .apply(lambda x: x.replace(np.NaN, np.nanmean(x), axis = 0))#\\\n",
    "#        .apply(zscore)\n",
    "    \n",
    "    # create interaction columns for continuous features\n",
    "    \n",
    "    interCols = [\n",
    "        ['stemLag1','stemLag2'],\n",
    "        #['stemLag1','stemLag2','lagTr1Tr2'],\n",
    "        #['lagTr1Tr2','morphoComplexity'],\n",
    "        #['morphoLag1','morphoComplexity'],\n",
    "    ]\n",
    "    td = interaction(td,interCols)\n",
    "    \n",
    "    # now for categoritcal * continuous\n",
    "    catInterCols = [\n",
    "        ['format', 'PhonNei'],\n",
    "        ['format', 'OrthoNei'],\n",
    "        ['pos', 'morphoComplexity']\n",
    "    ]\n",
    "    td = catInteraction(td,catInterCols)\n",
    "    \n",
    "    # now one-hot encode the categoricals & remove the originals from the df\n",
    "    catCols = ['countries', 'client','session', 'format','pos', 'format:prevFormat', 'format:client']+ morphos\n",
    "    for col in catCols:\n",
    "        oneHotCols = pd.get_dummies(td[col], prefix = '{'+col+'}', prefix_sep='')\n",
    "        td = td.drop([col],axis=1).join(oneHotCols)\n",
    "                                       \n",
    "    print('Complete.')\n",
    "    print('All done!')\n",
    "    return(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing users learning spanish...\n",
      "   Adding dataframe-level features ...Complete. \n",
      "Now processing user- & exercise-level data...\n",
      "Merging users' data & gathering orthographic info... \n",
      "Calculating morphological complexity\n",
      "Transforming, imputing missing values, and z-scoring numeric columns..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/ipykernel_py3/lib/python3.6/site-packages/pandas/core/generic.py:3786: UserWarning: the \"axis\" argument is deprecated and will be removed inv0.13; this argument has no effect\n",
      "  warn('the \"axis\" argument is deprecated and will be removed in'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete.\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# Process all 3 languages\n",
    "for l2 in langs:\n",
    "    data[l2+'_df'] = prepData(data[l2].copy(), l2).sort_values(by='origRow_doNotUse') # send a copy so that original data is preserved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Data/DuoData_processed_'+l2+'.pickle', 'wb') as handle:\n",
    "    pickle.dump(data, handle)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "notify_time": "5",
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "166px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
